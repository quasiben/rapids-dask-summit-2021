{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29126cf3-0734-4333-9cf6-b2354f1e941b",
   "metadata": {},
   "source": [
    "# RAPIDS + Dask NLP Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376de529-bb5f-4096-9fd8-40d9816c9964",
   "metadata": {},
   "source": [
    "In this notebook, we'll start by introducing a small subset of the Natural Language Processing capabilities that RAPIDS provides. We'll then demonstrate how you can combine RAPIDS with Dask to scale out these capabilities across many GPUs to process large datasets, and even do complex tasks like TF-IDF based similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829b16c5-f40d-47fb-900f-119ff3eb2528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed83853-c0e9-49f5-b7f2-286ba82e19e7",
   "metadata": {},
   "source": [
    "## Single GPU NLP Capabilities\n",
    "\n",
    "Let's analyze some coronavirus related tweets from April 1st, 2020."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0daec2-efea-4ab8-a7f2-3bb1fed70d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/raid/vjawa/string_exp/tweets/2020-04-01 Coronavirus Tweets.CSV\"\n",
    "df = cudf.read_csv(path)\n",
    "df = df.loc[df.lang == 'en']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cbe2c3-615f-49ac-afc9-602b9ba85b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e6960c-87af-4bad-b404-f1f74929faa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787386f3-52a2-4f60-9e54-daed8f69087f",
   "metadata": {},
   "source": [
    "Let's tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c16c85-8f8d-4a59-a110-b8566171ef94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.str.tokenize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f780a445-b208-4bd6-ac70-7f70479a7473",
   "metadata": {},
   "source": [
    "What are the most common tokens?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd19ac2-555d-4076-8148-5bef56035a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.text.str.tokenize().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495a9d0a-0578-4839-a12d-6c4b38a44531",
   "metadata": {},
   "source": [
    "Stopwords. Of course we need to handle these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900db70e-81b8-4764-8263-81953e61c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3883a453-e174-438e-a4d4-2fc802d093cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "(df\n",
    " .text\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.tokenize()\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0aafe1-3d5c-496a-b03b-8a3d51f09900",
   "metadata": {},
   "source": [
    "Case-sensitivity. Need to handle that too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462a25df-654c-41a5-a992-0f89d6747ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.tokenize()\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f302f8ed-1b9a-4491-8ff8-53477b467e98",
   "metadata": {},
   "source": [
    "Punctuation may be affecting the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1405ab7-aa5a-4a9c-bf51-c13ca1f375f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>',\n",
    "           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\~', '\\t','\\\\n',\"'\",\",\",'~' , '—']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1824bad0-f858-4708-b3bf-7ad31bcd1361",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.tokenize()\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b267f94-fcee-4afb-b12b-6abfe9b74011",
   "metadata": {},
   "source": [
    "Looks like web address terms are the most common now. That kind of makes sense. We should explicitly include these in our `STOPWORDS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6370166-1136-4d2c-ae73-192d7808feb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS += [\"co\", \"https\", \"com\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba318af-ddbc-483c-8126-f1c9d3bf2f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.tokenize()\n",
    ").value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1313b74-1431-454a-bd38-f5b8868a5591",
   "metadata": {},
   "source": [
    "Handling newlines and doing whitespace normalization is generally a good idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286eaefa-157d-44e9-9575-4019217f800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = (df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.normalize_spaces()\n",
    " .str.tokenize()\n",
    ").value_counts()\n",
    "\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b786f1-789e-4c70-8861-c3fe8797b0fc",
   "metadata": {},
   "source": [
    "We've got the most common tokens. What about bigrams or trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c0df6-6f38-4011-bb98-49df9a613289",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.normalize_spaces()\n",
    " .str.ngrams_tokenize(n=2, separator=\" \")\n",
    " .value_counts()\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2066d3-f695-4ebb-8b93-f9281393a98e",
   "metadata": {},
   "source": [
    "This makes sense. These sound like they could be terms used commonly in hashtags. What about trigrams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb844e-ea96-4cc6-8005-f0222b8b4f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "(df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.normalize_spaces()\n",
    " .str.ngrams_tokenize(n=3, separator=\" \")\n",
    " .value_counts()\n",
    ").head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d56eef1-86eb-46fd-b706-c5063814d475",
   "metadata": {},
   "source": [
    "RAPIDS provides an immense amount of NLP functionality, and what's particularly powerful is that we can take this into the Dask world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1ef930-114d-442b-a4c9-715d83f8038a",
   "metadata": {},
   "source": [
    "# Expanding to Larger, More Complex Tasks using Dask\n",
    "\n",
    "Let's touch on the previous example, and then move to something more complex like document search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef549bf-45d3-477c-8cda-c27d06fefc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db912ce-2c6c-49b3-97a8-428a83f70ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from dask.distributed import Client\n",
    "import dask.array as da\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "import cudf\n",
    "import dask_cudf\n",
    "import cupy as cp\n",
    "\n",
    "from cuml.dask.feature_extraction.text import TfidfTransformer\n",
    "from cuml.feature_extraction.text import HashingVectorizer as CumlHashVect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3e9b7-fc53-4816-9122-d84427756e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = LocalCUDACluster(\n",
    "    CUDA_VISIBLE_DEVICES=\"0,1,2,3\",\n",
    "    \n",
    ")\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c46a225-8801-46fd-b6b9-78e4e1ad1f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/raid/vjawa/string_exp/tweets/*.CSV\"\n",
    "df = dask_cudf.read_csv(path)\n",
    "\n",
    "df = df.loc[df.lang == 'en'].persist()\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d72eb7-572c-4c4f-80bd-dae249fb6581",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef60af9c-4e49-49d3-b6ce-82bade8a5a1d",
   "metadata": {},
   "source": [
    "## Tokenization (Again)\n",
    "\n",
    "We can do all the same processing we did before, this time using all of our GPU power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc60fa3-77ad-4f6d-b134-40685e74e887",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = nltk.corpus.stopwords.words('english')\n",
    "STOPWORDS += [\"co\", \"https\", \"com\"]\n",
    "\n",
    "PUNCTUATION = [ '!', '\"', '#', '$', '%', '&', '(', ')', '*', '+', '-', '.', '/',  '\\\\', ':', ';', '<', '=', '>',\n",
    "           '?', '@', '[', ']', '^', '_', '`', '{', '|', '}', '\\~', '\\t','\\\\n',\"'\",\",\",'~' , '—']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e30945c-41df-40c6-9db6-3bd97f8b3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code, using Dask this time to scale out to unlimited data\n",
    "\n",
    "results = (df\n",
    " .text\n",
    " .str.lower()\n",
    " .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    " .str.replace_tokens(STOPWORDS, \"\")\n",
    " .str.normalize_spaces()\n",
    " .str.tokenize()\n",
    ").value_counts()\n",
    "\n",
    "results.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9417ca0c-2312-417c-b1fc-4e0f8554a844",
   "metadata": {},
   "source": [
    "## Distributed TF-IDF Based Document Search\n",
    "\n",
    "Now that we know we can do these kinds of NLP operations with Dask, let's build a search tool using TF-IDF that lets us find tweets corresponding to our search query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6cfce4-2c86-412e-9b6e-cd9b9fe20b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CumlHashVect(stop_words='english')\n",
    "multi_gpu_transformer = TfidfTransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17d8102-d125-4940-bab9-cfab104a204a",
   "metadata": {},
   "source": [
    "Note that there is a `preprocessor` argument for the HashingVectorizer and it takes a callable. Let's actually redefine this with our own function, using the core logic from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e51c83-9cc8-4fa7-8501-ac781f4e57cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_preprocessor(s):\n",
    "    processed = (s\n",
    "                .str.lower()\n",
    "                .str.replace(PUNCTUATION, [\" \"]*len(PUNCTUATION), regex=False)\n",
    "                .str.replace_tokens(STOPWORDS, \"\")\n",
    "                .str.normalize_spaces()\n",
    "                )\n",
    "    return processed\n",
    "\n",
    "vectorizer = CumlHashVect(stop_words='english', preprocessor=our_preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c1c9b4-3d39-45de-8a7e-8060a2a27745",
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = da.from_array(cp.sparse.csr_matrix(cp.zeros(1, dtype=cp.float32)))\n",
    "X = df[\"text\"].map_partitions(vectorizer.fit_transform, meta=meta).astype(cp.float32)\n",
    "X = X.persist()\n",
    "X.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f8759e-3c87-4ac9-8619-648cdc8d66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_transformed = multi_gpu_transformer.fit_transform(X).persist()\n",
    "X_transformed.compute_chunk_sizes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f10e07-611a-472a-bb62-4c03b238e2b0",
   "metadata": {},
   "source": [
    "For simplicity, we'll collect our corpus and sparse tf-idf matrix to a single GPU and use the Dask multi-GPU vectorizer. This is not the most optimized approach, but it's simple and easy to walk through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ec5efc-9c8a-468d-9d18-24e64bf23379",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df[[\"text\", \"status_id\"]].compute()\n",
    "X_transformed_singlegpu = X_transformed.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2161b1-732e-4522-b4b1-a7d633015511",
   "metadata": {},
   "source": [
    "Using cuML's NearestNeighbors we can calculate the most similar records using Cosine Similarity on the sparse tf-idf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6f93a-9171-4a8d-8211-e497a91156e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.neighbors import NearestNeighbors\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=5, metric=\"cosine\")\n",
    "nn.fit(X_transformed_singlegpu)\n",
    "\n",
    "def search(haystack, needle):\n",
    "    query_vector = vectorizer.transform(cudf.Series(needle))\n",
    "    distances, indices = nn.kneighbors(query_vector)\n",
    "    return haystack.iloc[indices.ravel()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa91552-da13-4c7d-86b0-e28847265c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(corpus, \"NVIDIA AI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6eecbe-4298-4411-a201-99a80bebf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(corpus, \"distributed computing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0878ff39-a117-4a81-88bb-b68c6c8db03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "search(corpus, \"python programming gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df85229c-f84c-49a8-ba03-01e5c7fd4e6d",
   "metadata": {},
   "source": [
    "We've only scratched the surface of the NLP capabilities that Dask and RAPIDS make possible. We encourage you to look at the [RAPIDS](https://docs.rapids.ai/) and [Dask](https://docs.dask.org/en/latest/) documentation to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b998d88d-82c6-43cd-ae48-de9d1b2a6255",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
